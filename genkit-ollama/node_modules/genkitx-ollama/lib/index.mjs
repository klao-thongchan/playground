var __defProp = Object.defineProperty;
var __getOwnPropSymbols = Object.getOwnPropertySymbols;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __propIsEnum = Object.prototype.propertyIsEnumerable;
var __knownSymbol = (name, symbol) => {
  return (symbol = Symbol[name]) ? symbol : Symbol.for("Symbol." + name);
};
var __defNormalProp = (obj, key, value) => key in obj ? __defProp(obj, key, { enumerable: true, configurable: true, writable: true, value }) : obj[key] = value;
var __spreadValues = (a, b) => {
  for (var prop in b || (b = {}))
    if (__hasOwnProp.call(b, prop))
      __defNormalProp(a, prop, b[prop]);
  if (__getOwnPropSymbols)
    for (var prop of __getOwnPropSymbols(b)) {
      if (__propIsEnum.call(b, prop))
        __defNormalProp(a, prop, b[prop]);
    }
  return a;
};
var __async = (__this, __arguments, generator) => {
  return new Promise((resolve, reject) => {
    var fulfilled = (value) => {
      try {
        step(generator.next(value));
      } catch (e) {
        reject(e);
      }
    };
    var rejected = (value) => {
      try {
        step(generator.throw(value));
      } catch (e) {
        reject(e);
      }
    };
    var step = (x) => x.done ? resolve(x.value) : Promise.resolve(x.value).then(fulfilled, rejected);
    step((generator = generator.apply(__this, __arguments)).next());
  });
};
var __await = function(promise, isYieldStar) {
  this[0] = promise;
  this[1] = isYieldStar;
};
var __asyncGenerator = (__this, __arguments, generator) => {
  var resume = (k, v, yes, no) => {
    try {
      var x = generator[k](v), isAwait = (v = x.value) instanceof __await, done = x.done;
      Promise.resolve(isAwait ? v[0] : v).then((y) => isAwait ? resume(k === "return" ? k : "next", v[1] ? { done: y.done, value: y.value } : y, yes, no) : yes({ value: y, done })).catch((e) => resume("throw", e, yes, no));
    } catch (e) {
      no(e);
    }
  };
  var method = (k) => it[k] = (x) => new Promise((yes, no) => resume(k, x, yes, no));
  var it = {};
  return generator = generator.apply(__this, __arguments), it[__knownSymbol("asyncIterator")] = () => it, method("next"), method("throw"), method("return"), it;
};
var __forAwait = (obj, it, method) => (it = obj[__knownSymbol("asyncIterator")]) ? it.call(obj) : (obj = obj[__knownSymbol("iterator")](), it = {}, method = (key, fn) => (fn = obj[key]) && (it[key] = (arg) => new Promise((yes, no, done) => (arg = fn.call(obj, arg), done = arg.done, Promise.resolve(arg.value).then((value) => yes({ value, done }), no)))), method("next"), method("return"), it);
import {
  defineModel,
  GenerationCommonConfigSchema,
  getBasicUsageStats
} from "@genkit-ai/ai/model";
import { genkitPlugin } from "@genkit-ai/core";
import { logger } from "@genkit-ai/core/logging";
const ollama = genkitPlugin(
  "ollama",
  (params) => __async(void 0, null, function* () {
    const serverAddress = params == null ? void 0 : params.serverAddress;
    return {
      models: params.models.map(
        (model) => ollamaModel(model, serverAddress, params.requestHeaders)
      )
    };
  })
);
function ollamaModel(model, serverAddress, requestHeaders) {
  return defineModel(
    {
      name: `ollama/${model.name}`,
      label: `Ollama - ${model.name}`,
      configSchema: GenerationCommonConfigSchema,
      supports: {
        multiturn: !model.type || model.type === "chat"
      }
    },
    (input, streamingCallback) => __async(this, null, function* () {
      var _a, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l, _m;
      const options = {};
      if ((_a = input.config) == null ? void 0 : _a.hasOwnProperty("temperature")) {
        options.temperature = (_b = input.config) == null ? void 0 : _b.temperature;
      }
      if ((_c = input.config) == null ? void 0 : _c.hasOwnProperty("topP")) {
        options.top_p = (_d = input.config) == null ? void 0 : _d.topP;
      }
      if ((_e = input.config) == null ? void 0 : _e.hasOwnProperty("topK")) {
        options.top_k = (_f = input.config) == null ? void 0 : _f.topK;
      }
      if ((_g = input.config) == null ? void 0 : _g.hasOwnProperty("stopSequences")) {
        options.stop = (_i = (_h = input.config) == null ? void 0 : _h.stopSequences) == null ? void 0 : _i.join("");
      }
      if ((_j = input.config) == null ? void 0 : _j.hasOwnProperty("maxOutputTokens")) {
        options.num_predict = (_k = input.config) == null ? void 0 : _k.maxOutputTokens;
      }
      const type = (_l = model.type) != null ? _l : "chat";
      const request = toOllamaRequest(
        model.name,
        input,
        options,
        type,
        !!streamingCallback
      );
      logger.debug(request, `ollama request (${type})`);
      const extraHeaders = requestHeaders ? typeof requestHeaders === "function" ? yield requestHeaders(
        {
          serverAddress,
          model
        },
        input
      ) : requestHeaders : {};
      let res;
      try {
        res = yield fetch(
          serverAddress + (type === "chat" ? "/api/chat" : "/api/generate"),
          {
            method: "POST",
            body: JSON.stringify(request),
            headers: __spreadValues({
              "Content-Type": "application/json"
            }, extraHeaders)
          }
        );
      } catch (e) {
        const cause = e.cause;
        if (cause) {
          if (cause instanceof Error && ((_m = cause.message) == null ? void 0 : _m.includes("ECONNREFUSED"))) {
            cause.message += ". Make sure ollama server is running.";
          }
          throw cause;
        }
        throw e;
      }
      if (!res.body) {
        throw new Error("Response has no body");
      }
      const responseCandidates = [];
      if (streamingCallback) {
        const reader = res.body.getReader();
        const textDecoder = new TextDecoder();
        let textResponse = "";
        try {
          for (var iter = __forAwait(readChunks(reader)), more, temp, error; more = !(temp = yield iter.next()).done; more = false) {
            const chunk = temp.value;
            const chunkText = textDecoder.decode(chunk);
            const json = JSON.parse(chunkText);
            const message = parseMessage(json, type);
            streamingCallback({
              index: 0,
              content: message.content
            });
            textResponse += message.content[0].text;
          }
        } catch (temp) {
          error = [temp];
        } finally {
          try {
            more && (temp = iter.return) && (yield temp.call(iter));
          } finally {
            if (error)
              throw error[0];
          }
        }
        responseCandidates.push({
          index: 0,
          finishReason: "stop",
          message: {
            role: "model",
            content: [
              {
                text: textResponse
              }
            ]
          }
        });
      } else {
        const txtBody = yield res.text();
        const json = JSON.parse(txtBody);
        logger.debug(txtBody, "ollama raw response");
        responseCandidates.push({
          index: 0,
          finishReason: "stop",
          message: parseMessage(json, type)
        });
      }
      return {
        candidates: responseCandidates,
        usage: getBasicUsageStats(input.messages, responseCandidates)
      };
    })
  );
}
function parseMessage(response, type) {
  if (response.error) {
    throw new Error(response.error);
  }
  if (type === "chat") {
    return {
      role: toGenkitRole(response.message.role),
      content: [
        {
          text: response.message.content
        }
      ]
    };
  } else {
    return {
      role: "model",
      content: [
        {
          text: response.response
        }
      ]
    };
  }
}
function toOllamaRequest(name, input, options, type, stream) {
  const request = {
    model: name,
    options,
    stream
  };
  if (type === "chat") {
    const messages = [];
    input.messages.forEach((m) => {
      let messageText = "";
      const images = [];
      m.content.forEach((c) => {
        if (c.text) {
          messageText += c.text;
        }
        if (c.media) {
          images.push(c.media.url);
        }
      });
      messages.push({
        role: toOllamaRole(m.role),
        content: messageText,
        images: images.length > 0 ? images : void 0
      });
    });
    request.messages = messages;
  } else {
    request.prompt = getPrompt(input);
  }
  return request;
}
function toOllamaRole(role) {
  if (role === "model") {
    return "assistant";
  }
  return role;
}
function toGenkitRole(role) {
  if (role === "assistant") {
    return "model";
  }
  return role;
}
function readChunks(reader) {
  return {
    [Symbol.asyncIterator]() {
      return __asyncGenerator(this, null, function* () {
        let readResult = yield new __await(reader.read());
        while (!readResult.done) {
          yield readResult.value;
          readResult = yield new __await(reader.read());
        }
      });
    }
  };
}
function getPrompt(input) {
  return input.messages.map((m) => m.content.map((c) => c.text).join()).join();
}
export {
  ollama
};
//# sourceMappingURL=index.mjs.map